import { IntentionalShutdownCause, liveStoreVersion, makeClientSession, StoreInterrupted, UnexpectedError, } from '@livestore/common';
// TODO bring back - this currently doesn't work due to https://github.com/vitejs/vite/issues/8427
// NOTE We're using a non-relative import here for Vite to properly resolve the import during app builds
// import LiveStoreSharedWorker from '@livestore/adapter-web/internal-shared-worker?sharedworker'
import { EventSequenceNumber, SystemTables } from '@livestore/common/schema';
import { sqliteDbFactory } from '@livestore/sqlite-wasm/browser';
import { loadSqlite3Wasm } from '@livestore/sqlite-wasm/load-wasm';
import { isDevEnv, shouldNeverHappen, tryAsFunctionAndNew } from '@livestore/utils';
import { BrowserWorker, Cause, Deferred, Effect, Exit, Fiber, ParseResult, Queue, Schema, Stream, SubscriptionRef, WebLock, Worker, WorkerError, } from '@livestore/utils/effect';
import { nanoid } from '@livestore/utils/nanoid';
import * as OpfsUtils from '../../opfs-utils.js';
import { readPersistedAppDbFromClientSession, resetPersistedDataFromClientSession } from '../common/persisted-sqlite.js';
import { makeShutdownChannel } from '../common/shutdown-channel.js';
import { DedicatedWorkerDisconnectBroadcast, makeWorkerDisconnectChannel } from '../common/worker-disconnect-channel.js';
import * as WorkerSchema from '../common/worker-schema.js';
import { connectWebmeshNodeClientSession } from './client-session-devtools.js';
// NOTE we're starting to initialize the sqlite wasm binary here to speed things up
const sqlite3Promise = loadSqlite3Wasm();
if (isDevEnv()) {
    globalThis.__debugLiveStoreUtils = {
        opfs: OpfsUtils,
        runSync: (effect) => Effect.runSync(effect),
        runFork: (effect) => Effect.runFork(effect),
    };
}
/**
 * Creates a web adapter with persistent storage (currently only supports OPFS).
 * Requires both a web worker and a shared worker.
 *
 * @example
 * ```ts
 * import { makePersistedAdapter } from '@livestore/adapter-web'
 * import LiveStoreWorker from './livestore.worker?worker'
 * import LiveStoreSharedWorker from '@livestore/adapter-web/shared-worker?sharedworker'
 *
 * const adapter = makePersistedAdapter({
 *   worker: LiveStoreWorker,
 *   sharedWorker: LiveStoreSharedWorker,
 *   storage: { type: 'opfs' },
 * })
 * ```
 */
export const makePersistedAdapter = (options) => (adapterArgs) => Effect.gen(function* () {
    const { schema, storeId, devtoolsEnabled, debugInstanceId, bootStatusQueue, shutdown, syncPayload } = adapterArgs;
    yield* ensureBrowserRequirements;
    yield* Queue.offer(bootStatusQueue, { stage: 'loading' });
    const sqlite3 = yield* Effect.promise(() => sqlite3Promise);
    const LIVESTORE_TAB_LOCK = `livestore-tab-lock-${storeId}`;
    const storageOptions = yield* Schema.decode(WorkerSchema.StorageType)(options.storage);
    const shutdownChannel = yield* makeShutdownChannel(storeId);
    if (options.resetPersistence === true) {
        yield* shutdownChannel.send(IntentionalShutdownCause.make({ reason: 'adapter-reset' }));
        yield* resetPersistedDataFromClientSession({ storageOptions, storeId });
    }
    // Note on fast-path booting:
    // Instead of waiting for the leader worker to boot and then get a database snapshot from it,
    // we're here trying to get the snapshot directly from storage
    // we usually speeds up the boot process by a lot.
    // We need to be extra careful though to not run into any race conditions or inconsistencies.
    // TODO also verify persisted data
    const dataFromFile = options.experimental?.disableFastPath === true
        ? undefined
        : yield* readPersistedAppDbFromClientSession({ storageOptions, storeId, schema });
    // The same across all client sessions (i.e. tabs, windows)
    const clientId = options.clientId ?? getPersistedId(`clientId:${storeId}`, 'local');
    // Unique per client session (i.e. tab, window)
    const sessionId = options.sessionId ?? getPersistedId(`sessionId:${storeId}`, 'session');
    const workerDisconnectChannel = yield* makeWorkerDisconnectChannel(storeId);
    yield* shutdownChannel.listen.pipe(Stream.flatten(), Stream.tap((error) => shutdown(Cause.fail(error))), Stream.runDrain, Effect.interruptible, Effect.tapCauseLogPretty, Effect.forkScoped);
    const sharedWebWorker = tryAsFunctionAndNew(options.sharedWorker, { name: `livestore-shared-worker-${storeId}` });
    const sharedWorkerFiber = yield* Worker.makePoolSerialized({
        size: 1,
        concurrency: 100,
        initialMessage: () => new WorkerSchema.SharedWorker.InitialMessage({
            liveStoreVersion,
            payload: {
                _tag: 'FromClientSession',
                initialMessage: new WorkerSchema.LeaderWorkerInner.InitialMessage({
                    storageOptions,
                    storeId,
                    clientId,
                    devtoolsEnabled,
                    debugInstanceId,
                    syncPayload,
                }),
            },
        }),
    }).pipe(Effect.provide(BrowserWorker.layer(() => sharedWebWorker)), Effect.tapCauseLogPretty, UnexpectedError.mapToUnexpectedError, Effect.tapErrorCause(shutdown), Effect.withSpan('@livestore/adapter-web:client-session:setupSharedWorker'), Effect.forkScoped);
    const lockDeferred = yield* Deferred.make();
    // It's important that we resolve the leader election in a blocking way, so there's always a leader.
    // Otherwise events could end up being dropped.
    //
    // Sorry for this pun ...
    let gotLocky = yield* WebLock.tryGetDeferredLock(lockDeferred, LIVESTORE_TAB_LOCK);
    const lockStatus = yield* SubscriptionRef.make(gotLocky ? 'has-lock' : 'no-lock');
    // Ideally we can come up with a simpler implementation that doesn't require this
    const waitForSharedWorkerInitialized = yield* Deferred.make();
    if (gotLocky === false) {
        // Don't need to wait if we're not the leader
        yield* Deferred.succeed(waitForSharedWorkerInitialized, undefined);
    }
    const runLocked = Effect.gen(function* () {
        yield* Effect.logDebug(`[@livestore/adapter-web:client-session] ✅ Got lock '${LIVESTORE_TAB_LOCK}' (clientId: ${clientId}, sessionId: ${sessionId}).`);
        yield* Effect.addFinalizer(() => Effect.logDebug(`[@livestore/adapter-web:client-session] Releasing lock for '${LIVESTORE_TAB_LOCK}'`));
        yield* SubscriptionRef.set(lockStatus, 'has-lock');
        const mc = new MessageChannel();
        // NOTE we're adding the `storeId` to the worker name to make it unique
        // and adding the `sessionId` to make it easier to debug which session a worker belongs to in logs
        const worker = tryAsFunctionAndNew(options.worker, { name: `livestore-worker-${storeId}-${sessionId}` });
        yield* Worker.makeSerialized({
            initialMessage: () => new WorkerSchema.LeaderWorkerOuter.InitialMessage({ port: mc.port1, storeId, clientId }),
        }).pipe(Effect.provide(BrowserWorker.layer(() => worker)), UnexpectedError.mapToUnexpectedError, Effect.tapErrorCause(shutdown), Effect.withSpan('@livestore/adapter-web:client-session:setupDedicatedWorker'), Effect.tapCauseLogPretty, Effect.forkScoped);
        yield* workerDisconnectChannel.send(DedicatedWorkerDisconnectBroadcast.make({}));
        const sharedWorker = yield* Fiber.join(sharedWorkerFiber);
        yield* sharedWorker
            .executeEffect(new WorkerSchema.SharedWorker.UpdateMessagePort({ port: mc.port2 }))
            .pipe(UnexpectedError.mapToUnexpectedError, Effect.tapErrorCause(shutdown));
        yield* Deferred.succeed(waitForSharedWorkerInitialized, undefined);
        yield* Effect.never;
    }).pipe(Effect.withSpan('@livestore/adapter-web:client-session:lock'));
    // TODO take/give up lock when tab becomes active/passive
    if (gotLocky === false) {
        yield* Effect.logDebug(`[@livestore/adapter-web:client-session] ⏳ Waiting for lock '${LIVESTORE_TAB_LOCK}' (sessionId: ${sessionId})`);
        // TODO find a cleaner implementation for the lock handling as we don't make use of the deferred properly right now
        yield* WebLock.waitForDeferredLock(lockDeferred, LIVESTORE_TAB_LOCK).pipe(Effect.andThen(() => {
            gotLocky = true;
            return runLocked;
        }), Effect.interruptible, Effect.tapCauseLogPretty, Effect.forkScoped);
    }
    else {
        yield* runLocked.pipe(Effect.interruptible, Effect.tapCauseLogPretty, Effect.forkScoped);
    }
    const runInWorker = (req) => Fiber.join(sharedWorkerFiber).pipe(
    // NOTE we need to wait for the shared worker to be initialized before we can send requests to it
    Effect.tap(() => waitForSharedWorkerInitialized), Effect.flatMap((worker) => worker.executeEffect(req)), 
    // NOTE we want to treat worker requests as atomic and therefore not allow them to be interrupted
    // Interruption usually only happens during leader re-election or store shutdown
    // Effect.uninterruptible,
    Effect.logWarnIfTakesLongerThan({
        label: `@livestore/adapter-web:client-session:runInWorker:${req._tag}`,
        duration: 2000,
    }), Effect.withSpan(`@livestore/adapter-web:client-session:runInWorker:${req._tag}`), Effect.mapError((cause) => Schema.is(UnexpectedError)(cause)
        ? cause
        : ParseResult.isParseError(cause) || Schema.is(WorkerError.WorkerError)(cause)
            ? new UnexpectedError({ cause })
            : cause), Effect.catchAllDefect((cause) => new UnexpectedError({ cause })));
    const runInWorkerStream = (req) => Effect.gen(function* () {
        const sharedWorker = yield* Fiber.join(sharedWorkerFiber);
        return sharedWorker.execute(req).pipe(Stream.mapError((cause) => Schema.is(UnexpectedError)(cause)
            ? cause
            : ParseResult.isParseError(cause) || Schema.is(WorkerError.WorkerError)(cause)
                ? new UnexpectedError({ cause })
                : cause), Stream.withSpan(`@livestore/adapter-web:client-session:runInWorkerStream:${req._tag}`));
    }).pipe(Stream.unwrap);
    const bootStatusFiber = yield* runInWorkerStream(new WorkerSchema.LeaderWorkerInner.BootStatusStream()).pipe(Stream.tap((_) => Queue.offer(bootStatusQueue, _)), Stream.runDrain, Effect.tapErrorCause((cause) => (Cause.isInterruptedOnly(cause) ? Effect.void : shutdown(cause))), Effect.interruptible, Effect.tapCauseLogPretty, Effect.forkScoped);
    yield* Queue.awaitShutdown(bootStatusQueue).pipe(Effect.andThen(Fiber.interrupt(bootStatusFiber)), Effect.tapCauseLogPretty, Effect.forkScoped);
    // TODO maybe bring back transfering the initially created in-memory db snapshot instead of
    // re-exporting the db
    const initialResult = dataFromFile === undefined
        ? yield* runInWorker(new WorkerSchema.LeaderWorkerInner.GetRecreateSnapshot()).pipe(Effect.map(({ snapshot, migrationsReport }) => ({
            _tag: 'from-leader-worker',
            snapshot,
            migrationsReport,
        })))
        : { _tag: 'fast-path', snapshot: dataFromFile };
    const migrationsReport = initialResult._tag === 'from-leader-worker' ? initialResult.migrationsReport : { migrations: [] };
    const makeSqliteDb = sqliteDbFactory({ sqlite3 });
    const sqliteDb = yield* makeSqliteDb({ _tag: 'in-memory' });
    sqliteDb.import(initialResult.snapshot);
    const numberOfTables = sqliteDb.select(`select count(*) as count from sqlite_master`)[0]?.count ?? 0;
    if (numberOfTables === 0) {
        yield* UnexpectedError.make({
            cause: `Encountered empty or corrupted database`,
            payload: { snapshotByteLength: initialResult.snapshot.byteLength, storageOptions: options.storage },
        });
    }
    // We're restoring the leader head from the SESSION_CHANGESET_META_TABLE, not from the eventlog db/table
    // in order to avoid exporting/transferring the eventlog db/table, which is important to speed up the fast path.
    const initialLeaderHeadRes = sqliteDb.select(`select seqNumGlobal, seqNumClient from ${SystemTables.SESSION_CHANGESET_META_TABLE} order by seqNumGlobal desc, seqNumClient desc limit 1`)[0];
    const initialLeaderHead = initialLeaderHeadRes
        ? EventSequenceNumber.make({
            global: initialLeaderHeadRes.seqNumGlobal,
            client: initialLeaderHeadRes.seqNumClient,
        })
        : EventSequenceNumber.ROOT;
    // console.debug('[@livestore/adapter-web:client-session] initialLeaderHead', initialLeaderHead)
    yield* Effect.addFinalizer((ex) => Effect.gen(function* () {
        if (Exit.isFailure(ex) &&
            Exit.isInterrupted(ex) === false &&
            Schema.is(IntentionalShutdownCause)(Cause.squash(ex.cause)) === false &&
            Schema.is(StoreInterrupted)(Cause.squash(ex.cause)) === false) {
            yield* Effect.logError('[@livestore/adapter-web:client-session] client-session shutdown', ex.cause);
        }
        else {
            yield* Effect.logDebug('[@livestore/adapter-web:client-session] client-session shutdown', gotLocky, ex);
        }
        if (gotLocky) {
            yield* Deferred.succeed(lockDeferred, undefined);
        }
    }).pipe(Effect.tapCauseLogPretty, Effect.orDie));
    const leaderThread = {
        export: runInWorker(new WorkerSchema.LeaderWorkerInner.Export()).pipe(Effect.timeout(10_000), UnexpectedError.mapToUnexpectedError, Effect.withSpan('@livestore/adapter-web:client-session:export')),
        events: {
            pull: ({ cursor }) => runInWorkerStream(new WorkerSchema.LeaderWorkerInner.PullStream({ cursor })).pipe(Stream.orDie),
            push: (batch) => runInWorker(new WorkerSchema.LeaderWorkerInner.PushToLeader({ batch })).pipe(Effect.withSpan('@livestore/adapter-web:client-session:pushToLeader', {
                attributes: { batchSize: batch.length },
            })),
        },
        initialState: { leaderHead: initialLeaderHead, migrationsReport },
        getEventlogData: runInWorker(new WorkerSchema.LeaderWorkerInner.ExportEventlog()).pipe(Effect.timeout(10_000), UnexpectedError.mapToUnexpectedError, Effect.withSpan('@livestore/adapter-web:client-session:getEventlogData')),
        getSyncState: runInWorker(new WorkerSchema.LeaderWorkerInner.GetLeaderSyncState()).pipe(UnexpectedError.mapToUnexpectedError, Effect.withSpan('@livestore/adapter-web:client-session:getLeaderSyncState')),
        sendDevtoolsMessage: (message) => runInWorker(new WorkerSchema.LeaderWorkerInner.ExtraDevtoolsMessage({ message })).pipe(UnexpectedError.mapToUnexpectedError, Effect.withSpan('@livestore/adapter-web:client-session:devtoolsMessageForLeader')),
    };
    const sharedWorker = yield* Fiber.join(sharedWorkerFiber);
    const clientSession = yield* makeClientSession({
        ...adapterArgs,
        sqliteDb,
        lockStatus,
        clientId,
        sessionId,
        // isLeader: gotLocky, // TODO update when leader is changing
        isLeader: true,
        leaderThread,
        webmeshMode: 'direct',
        connectWebmeshNode: ({ sessionInfo, webmeshNode }) => connectWebmeshNodeClientSession({ webmeshNode, sessionInfo, sharedWorker, devtoolsEnabled, schema }),
        registerBeforeUnload: (onBeforeUnload) => {
            if (typeof window !== 'undefined' && typeof window.addEventListener === 'function') {
                window.addEventListener('beforeunload', onBeforeUnload);
                return () => window.removeEventListener('beforeunload', onBeforeUnload);
            }
            return () => { };
        },
    });
    return clientSession;
}).pipe(UnexpectedError.mapToUnexpectedError);
// NOTE for `local` storage we could also use the eventlog db to store the data
const getPersistedId = (key, storageType) => {
    const makeId = () => nanoid(5);
    const storage = typeof window === 'undefined'
        ? undefined
        : storageType === 'session'
            ? sessionStorage
            : storageType === 'local'
                ? localStorage
                : shouldNeverHappen(`[@livestore/adapter-web] Invalid storage type: ${storageType}`);
    // in case of a worker, we need the id of the parent window, to keep the id consistent
    // we also need to handle the case where there are multiple workers being spawned by the same window
    if (storage === undefined) {
        return makeId();
    }
    const fullKey = `livestore:${key}`;
    const storedKey = storage.getItem(fullKey);
    if (storedKey)
        return storedKey;
    const newKey = makeId();
    storage.setItem(fullKey, newKey);
    return newKey;
};
const ensureBrowserRequirements = Effect.gen(function* () {
    const validate = (condition, label) => Effect.gen(function* () {
        if (condition) {
            yield* UnexpectedError.make({
                cause: `[@livestore/adapter-web] Browser not supported. The LiveStore web adapter needs '${label}' to work properly`,
            });
        }
    });
    yield* Effect.all([
        validate(typeof navigator === 'undefined', 'navigator'),
        validate(navigator.locks === undefined, 'navigator.locks'),
        validate(navigator.storage === undefined, 'navigator.storage'),
        validate(typeof window === 'undefined', 'window'),
        validate(typeof sessionStorage === 'undefined', 'sessionStorage'),
    ]);
});
//# sourceMappingURL=persisted-adapter.js.map