import { gmFetchLLM,  gmFetchLLMStream } from "./gm-helper";

export interface OpenAIRequest {
  model: string;
  messages: { role: string; content: string }[];
  stream: boolean;
  maxTokens?: number;
  temperature?: number;
  top_p?: number;
}

// export interface LLMOptions {
//   maxTokens?: number;
//   temperature?: number;
//   top_p?: number;
//   contextWindow: number;
// }

export interface OpenAIOptions {
  provider?: string;
  baseURL: string;
  apiKey: string;
  data: OpenAIRequest;
  stream?: boolean;
}

export async function sendOpenAiRequest(
  options: OpenAIOptions,
  onChunk?: (chunk: string) => void
): Promise<string | void> {
  const isStreaming = options.stream === true && onChunk !== undefined;
  const requestData = { ...options.data, stream: isStreaming };
  
  // Ensure baseURL ends with /
  const normalizedBaseURL = options.baseURL.endsWith('/') ? options.baseURL : `${options.baseURL}/`;
  const apiURL = `${normalizedBaseURL}chat/completions`;
  
  // --- X·ª≠ l√Ω cho tr∆∞·ªùng h·ª£p KH√îNG STREAMING (gi·ªØ l·∫°i code c≈© n·∫øu c·∫ßn) ---
  if (!isStreaming) {
    // B·∫°n c√≥ th·ªÉ gi·ªØ l·∫°i h√†m gmFetchLLM c≈© cho tr∆∞·ªùng h·ª£p n√†y,
    // ho·∫∑c t·∫°o m·ªôt request m·ªõi ·ªü ƒë√¢y.
    // ho·∫∑c t·∫°o m·ªôt request m·ªõi ·ªü ƒë√¢y.
    
    const response = await gmFetchLLM(apiURL, { // Gi·∫£ s·ª≠ gmFetchLLM c≈© v·∫´n t·ªìn t·∫°i
      method: "POST",
      headers: {
        Authorization: `Bearer ${options.apiKey}`,
      },
      body: JSON.stringify(requestData),
    });
    const data = await response.json();
    
    if (!data) {
      console.error("‚ùå LLM Response is null/undefined");
      return '';
    }
    
    if (data.error) {
      console.error("‚ùå LLM API Error:", data.error);
      return '';
    }

    return data?.choices?.[0]?.message?.content || '';
  }

  // --- X·ª≠ l√Ω cho tr∆∞·ªùng h·ª£p STREAMING ---
  console.log("üé¨ STREAMING MODE ACTIVATED"); // Debug: Check if streaming is triggered
  
  if (!onChunk) {
    throw new Error('onChunk callback is required for streaming mode');
  }

  // Buffer ƒë·ªÉ l∆∞u tr·ªØ c√°c d√≤ng d·ªØ li·ªáu ch∆∞a ho√†n ch·ªânh
  let buffer = "";

  const processDataChunk = (newData: string) => {
    // N·ªëi d·ªØ li·ªáu m·ªõi v√†o buffer
    buffer += newData;

    // T√°ch buffer th√†nh c√°c d√≤ng
    const lines = buffer.split('\n');

    // D√≤ng cu·ªëi c√πng c√≥ th·ªÉ ch∆∞a ho√†n ch·ªânh, gi·ªØ l·∫°i n√≥ trong buffer cho l·∫ßn sau
    buffer = lines.pop() || "";

    // X·ª≠ l√Ω t·ª´ng d√≤ng ho√†n ch·ªânh
    for (const line of lines) {
      const trimmedLine = line.trim();
      if (trimmedLine.startsWith("data: ")) {
        const jsonStr = trimmedLine.slice(5).trim();
        if (jsonStr === "[DONE]") {
          // Stream ƒë√£ k·∫øt th√∫c t·ª´ ph√≠a server
          return;
        }
        try {
          const data = JSON.parse(jsonStr);
          const content = data?.choices?.[0]?.delta?.content;
          if (content) {
            // G·ª≠i n·ªôi dung ƒë·∫øn callback cu·ªëi c√πng c·ªßa ng∆∞·ªùi d√πng
            onChunk(content);
          }
        } catch (e) {
          console.warn("Could not parse streaming JSON chunk:", jsonStr, e);
        }
      }
    }
  };

  try {
    // G·ªçi h√†m streaming m·ªõi
    await gmFetchLLMStream(apiURL, {
      method: "POST",
      headers: {
        "Content-Type": "application/json",
        Authorization: `Bearer ${options.apiKey}`,
      },
      body: JSON.stringify(requestData),
      onChunk: processDataChunk, // Cung c·∫•p h√†m x·ª≠ l√Ω chunk
    });
    // Khi promise resolve, stream ƒë√£ k·∫øt th√∫c
    console.log("LLM stream finished successfully.");
  } catch (error) {
    console.error("Error during LLM stream:", error);
    // B√°o l·ªói cho ng∆∞·ªùi d√πng, v√≠ d·ª•: onChunk("[ERROR]...")
    onChunk(`\n\n[L·ªñI]: ${(error as Error).message}`);
    throw error;
  }
}

// Convenience function for non-streaming requests
export async function sendOpenAiRequestSync(
  options: Omit<OpenAIOptions, 'stream'>
): Promise<string> {
  const result = await sendOpenAiRequest({ ...options, stream: false });
  return result as string;
}

// Convenience function for streaming requests
export async function sendOpenAiRequestStream(
  options: Omit<OpenAIOptions, 'stream'>,
  onChunk: (chunk: string) => void
): Promise<void> {
  await sendOpenAiRequest({ ...options, stream: true }, onChunk);
}