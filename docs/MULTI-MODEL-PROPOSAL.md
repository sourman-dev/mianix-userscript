# ƒê·ªÅ xu·∫•t: Multi-Model System cho RAG

## üéØ V·∫•n ƒë·ªÅ hi·ªán t·∫°i

Hi·ªán t·∫°i **1 model default** ƒëang l√†m **3 vi·ªác kh√°c nhau**:

```typescript
const model = db.LLMModels.findOne({ isDefault: true });

// Model n√†y ph·∫£i:
1. üí¨ Response chat          ‚Üí C·∫ßn model m·∫°nh (GPT-4, Claude)
2. üß† Tr√≠ch xu·∫•t k√Ω ·ª©c       ‚Üí C·∫ßn model nhanh/r·∫ª (GPT-3.5)
3. üî¢ T·∫°o embedding vector   ‚Üí C·∫ßn model embedding chuy√™n d·ª•ng
```

### ‚ö†Ô∏è V·∫•n ƒë·ªÅ:
- **Embedding SAI**: GPT-4 kh√¥ng ph·∫£i embedding model!
- **T·ªën k√©m**: D√πng GPT-4 ƒë·ªÉ extract memories = l√£ng ph√≠
- **Ch·∫≠m**: Extraction ch·∫°y background nh∆∞ng v·∫´n t·ªën th·ªùi gian

---

## üí° Gi·∫£i ph√°p: 3 Model Types

### 1Ô∏è‚É£ **Chat Model** (Response)
- **M·ª•c ƒë√≠ch**: Tr·∫£ l·ªùi ng∆∞·ªùi d√πng
- **V√≠ d·ª•**: GPT-4, Claude-3, Gemini-Pro
- **ƒê·∫∑c ƒëi·ªÉm**: M·∫°nh, s√°ng t·∫°o, hi·ªÉu context t·ªët
- **Cost**: Cao ($$$)

### 2Ô∏è‚É£ **Extraction Model** (Ph√¢n t√≠ch)
- **M·ª•c ƒë√≠ch**: Tr√≠ch xu·∫•t k√Ω ·ª©c t·ª´ h·ªôi tho·∫°i
- **V√≠ d·ª•**: GPT-3.5-turbo, Gemini-Flash, Llama-3-8B
- **ƒê·∫∑c ƒëi·ªÉm**: Nhanh, r·∫ª, ƒë·ªß th√¥ng minh
- **Cost**: Th·∫•p ($)

### 3Ô∏è‚É£ **Embedding Model** (Vector)
- **M·ª•c ƒë√≠ch**: Chuy·ªÉn text ‚Üí vector s·ªë
- **V√≠ d·ª•**: `text-embedding-3-small`, `nomic-embed-text`
- **ƒê·∫∑c ƒëi·ªÉm**: Chuy√™n d·ª•ng, r·∫•t nhanh, r·∫•t r·∫ª
- **Cost**: R·∫•t th·∫•p (¬¢)

---

## üîß Implementation Plan

### B∆∞·ªõc 1: C·∫≠p nh·∫≠t Database Schema

```typescript
// src/db/index.ts

export type ModelType = 'chat' | 'embedding' | 'extraction';

export type LLMModel = {
  id: string;
  name: string;
  apiKey: string;
  baseUrl: string;
  modelName: string;
  llmProvider: string;
  isDefault: boolean;
  modelType: ModelType; // üÜï TH√äM M·ªöI
  createdAt: number;
};
```

### B∆∞·ªõc 2: C·∫≠p nh·∫≠t UI Modal

```typescript
// src/components/llm_models/Modal.vue

// Th√™m dropdown ch·ªçn Model Type
<Select 
  v-model="modelType" 
  :options="modelTypeOptions" 
  optionLabel="label" 
  optionValue="value"
  inputId="modelType" 
  class="w-full" 
/>

const modelTypeOptions = [
  { label: 'üí¨ Chat (Response)', value: 'chat' },
  { label: 'üß† Extraction (Ph√¢n t√≠ch)', value: 'extraction' },
  { label: 'üî¢ Embedding (Vector)', value: 'embedding' }
];
```

### B∆∞·ªõc 3: Helper Functions

```typescript
// src/utils/model-helpers.ts (NEW FILE)

import { db, LLMModel, ModelType } from '@/db';

export function getModelByType(type: ModelType): LLMModel | null {
  // T√¨m model theo type, ∆∞u ti√™n isDefault
  const defaultModel = db.LLMModels.findOne({ 
    modelType: type, 
    isDefault: true 
  });
  
  if (defaultModel) return defaultModel;
  
  // Fallback: L·∫•y model ƒë·∫ßu ti√™n c√≥ type n√†y
  return db.LLMModels.findOne({ modelType: type });
}

export function getChatModel(): LLMModel | null {
  return getModelByType('chat');
}

export function getExtractionModel(): LLMModel | null {
  // Fallback v·ªÅ chat model n·∫øu kh√¥ng c√≥ extraction model
  return getModelByType('extraction') || getChatModel();
}

export function getEmbeddingModel(): LLMModel | null {
  return getModelByType('embedding');
}
```

### B∆∞·ªõc 4: C·∫≠p nh·∫≠t Memory Service

```typescript
// src/services/memory-service.ts

import { getChatModel, getExtractionModel, getEmbeddingModel } from '@/utils/model-helpers';

export class MemoryService {
  
  // 1. T·∫°o Embedding - D√πng EMBEDDING MODEL
  static async generateEmbedding(text: string): Promise<number[]> {
    const embeddingModel = getEmbeddingModel();
    
    if (!embeddingModel) {
      console.error('‚ùå No embedding model configured!');
      return [];
    }
    
    try {
      const embedUrl = embeddingModel.baseUrl.endsWith('/') 
        ? `${embeddingModel.baseUrl}embeddings` 
        : `${embeddingModel.baseUrl}/embeddings`;
        
      const response = await fetch(embedUrl, {
        method: "POST",
        headers: {
          "Content-Type": "application/json",
          "Authorization": `Bearer ${embeddingModel.apiKey}`,
        },
        body: JSON.stringify({
          input: text,
          model: embeddingModel.modelName // D√πng model embedding chuy√™n d·ª•ng
        }),
      });
      
      const data = await response.json();
      return data.data?.[0]?.embedding || [];
    } catch (e) {
      console.error("Embedding generation failed:", e);
      return [];
    }
  }

  // 2. Tr√≠ch xu·∫•t k√Ω ·ª©c - D√πng EXTRACTION MODEL
  static async extractMemories(
    characterId: string,
    userMessage: string,
    aiMessage: string
  ) {
    const extractionModel = getExtractionModel();
    
    if (!extractionModel) {
      console.warn('‚ö†Ô∏è No extraction model, skipping memory extraction');
      return;
    }
    
    // ... logic extraction v·ªõi extractionModel
    const response = await sendOpenAiRequestSync({
      baseURL: extractionModel.baseUrl,
      apiKey: extractionModel.apiKey,
      data: {
        model: extractionModel.modelName, // D√πng model extraction (r·∫ª h∆°n)
        // ...
      }
    });
    
    // ... l∆∞u memories
  }

  // 3. Retrieval - D√πng EMBEDDING MODEL
  static async retrieveRelevantMemories(
    characterId: string,
    query: string,
    limit: number = 5
  ): Promise<string> {
    const embeddingModel = getEmbeddingModel();
    
    if (!embeddingModel) {
      console.warn('‚ö†Ô∏è No embedding model, skipping memory retrieval');
      return '';
    }
    
    const queryEmbedding = await this.generateEmbedding(query);
    // ... logic retrieval
  }
}
```

### B∆∞·ªõc 5: C·∫≠p nh·∫≠t Dialogue Store

```typescript
// src/stores/dialogue.ts

import { getExtractionModel, getEmbeddingModel } from '@/utils/model-helpers';

export const useDialogueStore = defineStore("dialogue", {
  actions: {
    async prepareContext(userInput: string) {
      if (!this.currentDialogue) return;
      
      // Kh√¥ng c·∫ßn truy·ªÅn model n·ªØa, service t·ª± l·∫•y
      this.relevantMemories = await MemoryService.retrieveRelevantMemories(
        this.currentDialogue.id,
        userInput
      );
    },
    
    async handlePostResponseProcess(userInput: string, aiResponse: string) {
      if (!this.currentDialogue) return;
      
      // Kh√¥ng c·∫ßn truy·ªÅn model n·ªØa
      MemoryService.extractMemories(
        this.currentDialogue.id,
        userInput,
        aiResponse
      );
    }
  }
});
```

---

## üìä So s√°nh Cost

### Tr∆∞·ªõc (1 model cho t·∫•t c·∫£):
```
Chat:       GPT-4 ($0.03/1K tokens)
Extraction: GPT-4 ($0.03/1K tokens) ‚ùå L√£ng ph√≠!
Embedding:  GPT-4 (KH√îNG H·ªñ TR·ª¢!) ‚ùå SAI!

‚Üí Total: R·∫•t ƒë·∫Øt + Kh√¥ng ho·∫°t ƒë·ªông
```

### Sau (3 models chuy√™n d·ª•ng):
```
Chat:       GPT-4         ($0.03/1K tokens)
Extraction: GPT-3.5-turbo ($0.0015/1K tokens) ‚úÖ R·∫ª h∆°n 20x
Embedding:  text-embed-3  ($0.00002/1K tokens) ‚úÖ R·∫ª h∆°n 1500x

‚Üí Total: Ti·∫øt ki·ªám ~90% chi ph√≠ cho RAG
```

---

## üéØ Migration Plan

### Phase 1: Backward Compatible (Tu·∫ßn 1)
1. ‚úÖ Th√™m field `modelType` (optional, default = 'chat')
2. ‚úÖ T·∫°o helper functions
3. ‚úÖ C·∫≠p nh·∫≠t Memory Service ƒë·ªÉ d√πng helpers
4. ‚úÖ Test v·ªõi 1 model (v·∫´n ho·∫°t ƒë·ªông nh∆∞ c≈©)

### Phase 2: UI Update (Tu·∫ßn 2)
1. ‚úÖ Th√™m dropdown Model Type v√†o Modal
2. ‚úÖ C·∫≠p nh·∫≠t LLMIndex ƒë·ªÉ hi·ªÉn th·ªã type
3. ‚úÖ Cho ph√©p user t·∫°o nhi·ªÅu models v·ªõi types kh√°c nhau

### Phase 3: Full Deployment (Tu·∫ßn 3)
1. ‚úÖ T·∫°o 3 models m·∫´u (chat + extraction + embedding)
2. ‚úÖ Documentation cho user
3. ‚úÖ Monitor performance

---

## üöÄ Quick Start (Sau khi implement)

### Setup 3 Models:

```typescript
// 1. Chat Model (Response)
{
  name: "GPT-4 Chat",
  modelType: "chat",
  modelName: "gpt-4-turbo-preview",
  isDefault: true
}

// 2. Extraction Model (Ph√¢n t√≠ch)
{
  name: "GPT-3.5 Extraction",
  modelType: "extraction",
  modelName: "gpt-3.5-turbo",
  isDefault: false
}

// 3. Embedding Model (Vector)
{
  name: "OpenAI Embedding",
  modelType: "embedding",
  modelName: "text-embedding-3-small",
  isDefault: false
}
```

---

## ‚úÖ Checklist Implementation

- [ ] C·∫≠p nh·∫≠t `LLMModel` type v·ªõi `modelType`
- [ ] T·∫°o `model-helpers.ts` v·ªõi getter functions
- [ ] C·∫≠p nh·∫≠t `Modal.vue` v·ªõi Model Type selector
- [ ] C·∫≠p nh·∫≠t `LLMIndex.vue` ƒë·ªÉ hi·ªÉn th·ªã type
- [ ] C·∫≠p nh·∫≠t `memory-service.ts` ƒë·ªÉ d√πng ƒë√∫ng model
- [ ] C·∫≠p nh·∫≠t `dialogue.ts` ƒë·ªÉ kh√¥ng truy·ªÅn model
- [ ] Migration script cho models c≈©
- [ ] Documentation
- [ ] Testing

---

## üìù Notes

### V·ªÅ Embedding:
- **ƒê√∫ng**: Vector embedding l√† m·∫£ng s·ªë (float[])
- **Sai**: D√πng GPT-4 ƒë·ªÉ t·∫°o embedding (n√≥ kh√¥ng h·ªó tr·ª£!)
- **ƒê√∫ng**: D√πng model embedding chuy√™n d·ª•ng nh∆∞ `text-embedding-3-small`

### V·ªÅ API Endpoints:
- Chat: `/v1/chat/completions`
- Embedding: `/v1/embeddings` (kh√°c endpoint!)

### V·ªÅ Ollama:
- Chat: `llama3`, `mistral`
- Embedding: `nomic-embed-text`, `mxbai-embed-large`

---

**B·∫°n mu·ªën t√¥i implement ngay kh√¥ng?** üöÄ
